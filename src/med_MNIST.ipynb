{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941cd3c2-2771-43b6-9a20-486f0f7d5bdc",
   "metadata": {},
   "source": [
    "# Model based on CNN with EDA\n",
    " \n",
    "**Author:** [Erik Matoviƒç](https://github.com/Matovic)  \n",
    "**Dataset:** https://www.kaggle.com/andrewmvd/medical-mnist [1]   \n",
    "## Exploratory Data Analysis\n",
    "\n",
    "### Choosing random data from each class to display\n",
    "\n",
    " - in this section, inspiration comes from the article [Exploratory Data Analysis Ideas for Image Classification](https://towardsdatascience.com/exploratory-data-analysis-ideas-for-image-classification-d3fc6bbfb2d2) [2]\n",
    " - first of all, we need to get the names of all subfolders in our data folder:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52dc20bd-54d7-4c27-8653-e4c68d919c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hand', 'AbdomenCT', 'HeadCT', 'train', 'BreastMRI', 'ChestCT', 'CXR', 'test']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '../data/' # image folder\n",
    "\n",
    "data_folders = [name for name in os.listdir(data_dir) if os.path.isdir(data_dir + name)]\n",
    "data_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca840e07-01fd-4202-bbf8-8bed115df8e1",
   "metadata": {},
   "source": [
    " - we take a look at three randomly chosen images and count the total number of our data \n",
    " - we have 58954 medical images as our data \n",
    " - the only class with less than ten-thousand medical images is the BreastMRI class\n",
    " - the color space of images is grayscale(8-bits), L mode in the instance of PIL image\n",
    " - all randomly chosen medical images have the same height and width, which both of them are 100 pixels\n",
    " - the output of the cell also contains additional information about the randomly chosen medical images\n",
    " - some medical images appear to have a lower quality by raw comparison\n",
    " - the proof of the lower quality of some medical images is with the minimal and maximum pixels; some medical images use the full range of the eight-bit grayscale channel(256 values ranging from 0 to 255), which have more quality\n",
    " - however, some medical images have the minimum value of the pixel around 100, which means the quality of the black color of the background has less quality in comparison with the minimal value set on the zero, and some medical images do not have the maximum value set on 255, which means the saturation of white color has less quality in comparison with the maximum value set on the 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d7ad53",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_folders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23326/2617745885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# get data from each folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_folders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# skip train and test folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_folders' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "%matplotlib inline\n",
    "\n",
    "all_data = 0 # counter\n",
    "classes = dict()\n",
    "\n",
    "# get data from each folder\n",
    "for folder in data_folders:\n",
    "    # skip train and test folders\n",
    "    if (folder == \"train\" or folder == \"test\"):\n",
    "        continue\n",
    "    \n",
    "    # get the list of jpegs from sub image class folders\n",
    "    imgs = [fn for fn in os.listdir(data_dir + folder) if fn.endswith('.jpeg')]\n",
    "\n",
    "    # randomly select 3 images from folder\n",
    "    random_select = np.random.choice(imgs, 3, replace = False)\n",
    "    \n",
    "    # plotting 1 x 3 image matrix\n",
    "    fig = plt.figure(figsize = (8,6))\n",
    "    for i in range(3):\n",
    "        fp = data_dir + folder + '/' + random_select[i]\n",
    "        ax = fig.add_subplot(1, 3, i+1)\n",
    "\n",
    "        # to plot without rescaling, remove target_size\n",
    "        fn = image.load_img(fp, target_size = (100,100), color_mode='grayscale')\n",
    "        print(\"Image's color model is:\", fn.mode)\n",
    "        print(\"Image's size is(width, height):\", fn.size)\n",
    "        print(\"Image's info:\", fn.info)\n",
    "        print(\"Image's pixel values(min, max):\", fn.getextrema())\n",
    "        plt.imshow(fn, cmap='Greys_r')\n",
    "        plt.title(folder)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # also check the number of files here\n",
    "    all_data += len(imgs)\n",
    "    classes[folder] = len(imgs)\n",
    "    print(f'{data_dir + folder} has {len(imgs)} files')\n",
    "\n",
    "print(f'We have {all_data} images in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa5303-ed46-42f7-8d92-4f59050519b1",
   "metadata": {},
   "source": [
    "### The number of medical images by individual classes\n",
    "\n",
    " - in this subsection of the exploratory data analysis, we have graphically represented the number of medical images by individual classes, classes of Hand, AbdomenCT, HradCT, ChestCT, and CXR have the same amount of data, which is ten-thousand medical images, and the only class with fewer medical images is BreastMRI class with 8954 medical images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc1727c-1280-44a0-b887-83c552a2d95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "keys = list(classes.keys())\n",
    "\n",
    "no_img = [classes[key] for key in keys] # get number of images in the same order as keys\n",
    "sns.barplot(x=keys, y=no_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa61ac3-4749-4638-96b0-3bce65e9f132",
   "metadata": {},
   "source": [
    "### Dividing a dataset into a training and test subset \n",
    "- before we start making our model, it is essential to divide our dataset into the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515eb1c4-79f7-4e4a-aa93-e5aa932eef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "for folder in data_folders:\n",
    "    # skip train and test folders\n",
    "    if (folder == \"train\" or folder == \"test\"):\n",
    "        continue\n",
    "        \n",
    "    # if directory exist, skip\n",
    "    if (os.path.isdir(data_dir +'train/' + folder)):\n",
    "        continue\n",
    "        \n",
    "    # otherwise create one\n",
    "    os.makedirs(data_dir +'train/' + folder)\n",
    "    os.makedirs(data_dir +'test/' + folder)\n",
    "    source_dir = data_dir + folder\n",
    "    allFileNames = os.listdir(source_dir)\n",
    "    np.random.shuffle(allFileNames)\n",
    "    \n",
    "    # 20% of our medical images is test data\n",
    "    test_ratio = 0.20\n",
    "\n",
    "    train_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                          [int(len(allFileNames)* (1 - test_ratio))])\n",
    "    \n",
    "    train_FileNames = [source_dir + '/' + name for name in train_FileNames.tolist()]\n",
    "    test_FileNames = [source_dir + '/' + name for name in test_FileNames.tolist()]\n",
    "    \n",
    "    for name in train_FileNames:\n",
    "      shutil.copy(name, data_dir +'train/' + folder)\n",
    "    for name in test_FileNames:\n",
    "      shutil.copy(name, data_dir +'test/' + folder)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53722b-d745-43a7-ac60-30a08b0d6a74",
   "metadata": {},
   "source": [
    " - we divided the data into the training data with 80% of the total medical images and testing data with 20% of the total medical images \n",
    " - additional information within the exploratory data analysis:\n",
    "     - images dimension: 64x64\n",
    "     - 47163 images represent 80% of our dataset, which is within the training subset, 20% of our dataset is in the testing subset\n",
    "     - six classes in the classification: Hand, AbdomenCT, HeadCT, BreastMRI, ChestCT, CXR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ecc256-7759-4f59-9c6c-5d931465498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47163 images for training in 6 classes\n",
      "There are 11791 images for testing in 6 classes\n",
      "Label names: ['Hand', 'AbdomenCT', 'HeadCT', 'BreastMRI', 'ChestCT', 'CXR']\n",
      "Label counts train: [8000, 8000, 8000, 7163, 8000, 8000]\n",
      "Label counts test: [2000, 2000, 2000, 1791, 2000, 2000]\n",
      "Image dimensions: 64 x 64\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# set paths for the dataset\n",
    "train_dir = '../data/train/'\n",
    "test_dir = '../data/test/'\n",
    "\n",
    "# get class names, equal as data_folders upper\n",
    "class_names = os.listdir(train_dir)  \n",
    "class_names_test = os.listdir(test_dir)  \n",
    "\n",
    "# get a number of classes, which is a number of subdirectories\n",
    "no_classes = len(class_names)        \n",
    "\n",
    "# get list of images' filenames based on classes\n",
    "# list has lenght of no_classes and each index have imgs of a class\n",
    "images_classes = list(range(no_classes))\n",
    "for i in range(0, no_classes, 1):\n",
    "    images_classes[i] = list()\n",
    "    for x in os.listdir(os.path.join(train_dir, class_names[i])):\n",
    "        images_classes[i].append(os.path.join(train_dir, class_names[i], x))\n",
    "\n",
    "        \n",
    "images_classes_test = list(range(no_classes))\n",
    "for i in range(0, no_classes, 1):\n",
    "    images_classes_test[i] = list()\n",
    "    for x in os.listdir(os.path.join(test_dir, class_names_test[i])):\n",
    "        images_classes_test[i].append(os.path.join(test_dir, class_names_test[i], x))\n",
    "            \n",
    "# get number of images of each class\n",
    "no_imgs_class = list()     \n",
    "for folder in range(0, no_classes, 1):\n",
    "    no_imgs_class.append(len(images_classes[folder]))\n",
    "    \n",
    "no_imgs_class_test = list()\n",
    "for folder in range(0, no_classes, 1):\n",
    "    no_imgs_class_test.append(len(images_classes_test[folder]))\n",
    "    \n",
    "# get list of filenames for testing & training\n",
    "imgs_files = list()           \n",
    "img_files_test = list()\n",
    "\n",
    "# get the labels or the type of each individual img in the list\n",
    "img_class = list()              \n",
    "img_class_test = list()\n",
    "\n",
    "# set variables of img_class & imgs_files\n",
    "for folder in range(0, no_classes, 1):\n",
    "    imgs_files.extend(images_classes[folder])\n",
    "    img_class.extend([folder] * no_imgs_class[folder])\n",
    "    \n",
    "# set variables for training\n",
    "for folder in range(0, no_classes, 1):\n",
    "    img_files_test.extend(images_classes_test[folder])\n",
    "    img_class_test.extend([folder] * no_imgs_class_test[folder])\n",
    "\n",
    "# get total number of images\n",
    "no_total_img = len(img_class)\n",
    "no_total_img_train = len(img_class_test)\n",
    "\n",
    "# get the dimensions\n",
    "img_width, img_height = Image.open(imgs_files[0]).size         \n",
    "\n",
    "print(\"There are\", no_total_img, \"images for training in\", no_classes, \"classes\")\n",
    "print(\"There are\", no_total_img_train, \"images for testing in\", no_classes, \"classes\")\n",
    "print(\"Label names:\", class_names)\n",
    "print(\"Label counts train:\", no_imgs_class)\n",
    "print(\"Label counts test:\", no_imgs_class_test)\n",
    "print(\"Image dimensions:\", img_width, \"x\", img_height)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf35a34-29d7-4e71-a17f-28e5fdf5b8a8",
   "metadata": {},
   "source": [
    " - classes Hand, AbdomenCT, HradCT, ChestCT, and CXR have the same number of data, eight thousand medical images, and a single class, BreastMRI, has fewer images, just over seven thousand medical data, together we have 47163 medical images in our training subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcfe9abc-3b38-47d2-8dae-0816b2ae8346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=class_names, y=no_imgs_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de335d7b-c7d6-4323-b5f2-25f1dca7a6bd",
   "metadata": {},
   "source": [
    " - classes Hand, AbdomenCT, HradCT, ChestCT, and CXR have the same number of data, two thousand medical images, and a single class, BreastMRI, has fewer images, just over one thousand medical data, together we have 47163 medical images in our testing subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003f2051-d602-493a-afbb-905c582d10c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=class_names, y=no_imgs_class_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230d5d5-d7b7-495c-ac3b-20e449baf2c9",
   "metadata": {},
   "source": [
    "### Model design\n",
    "\n",
    " - concerning 6 classes and medical images, we decided to design a classifier model based on a convolutional neural network, which is used to classify or segment image data\n",
    " - from the exploratory data analysis, we see that it will be necessary to transform the pixel values to the same scale because currently, not all pixel values of medical images start and end at the same values\n",
    " \n",
    " ## Model\n",
    "\n",
    " - to implement the model, we decided to use the PyTorch module using the CUDA architecture to parallelize the calculations on the GPU\n",
    " - if the machine does not have a graphics processing unit that supports the CUDA architecture, then the calculations will run on the CPU, which is slower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "732ffe48-2317-4e3c-830f-7c2bc655618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 {'num_workers': 8, 'pin_memory': True} cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as mp\n",
    "import torch\n",
    "import torchvision \n",
    "\n",
    "# run PyTorch on CUDA architecture\n",
    "if torch.cuda.is_available():     \n",
    "    dev = torch.device(\"cuda:0\")\n",
    "    kwar = {'num_workers': 8, 'pin_memory': True}\n",
    "    cpu = torch.device(\"cpu\")\n",
    "else:\n",
    "    print(\"Warning: CUDA not found, CPU only.\")\n",
    "    dev = torch.device(\"cpu\")\n",
    "    kwar = {}\n",
    "    cpu = torch.device(\"cpu\")\n",
    "\n",
    "print(dev, kwar, cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3edfbd-3be1-4121-b569-db1c47f0991a",
   "metadata": {},
   "source": [
    "- display which PyTorch version are we using\n",
    "- we have used PyTorch with version 1.10.1 and with CUDA architecture 11.3(1.10.1+cu113) on our local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f0200d1-2fdb-4cf6-8625-b022d13930c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu113\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51002be3-c28d-46d3-9628-214e5ae0ead7",
   "metadata": {},
   "source": [
    " - to properly extract information from pixels due to the grayscale color model, we increase the contrast and therefore scale the images to pixel values from 0 to 1\n",
    " - simultaneously, it is essential to convert JPEG images to data type tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db853c0c-889c-42f4-9da2-ded2097cd0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After rescaling, min pixel value: 0.0 and max pixel value: 1.0\n"
     ]
    }
   ],
   "source": [
    "toTensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "# Load, scale, and stack image tensor\n",
    "tensors = list()\n",
    "for img_path in imgs_files:\n",
    "    PIL_img = Image.open(img_path)\n",
    "    # from PIL image, get a tensor\n",
    "    tensor_img = toTensor(PIL_img)\n",
    "    \n",
    "    # the image isn't empty, rescale its values to run from 0 to 1\n",
    "    if(tensor_img.min() < tensor_img.max()):  \n",
    "        tensor_img = (tensor_img - tensor_img.min()) / (tensor_img.max() - tensor_img.min()) \n",
    "    \n",
    "    # append list of tensors\n",
    "    tensors.append(tensor_img)\n",
    "    \n",
    "# stack image (X) tensor\n",
    "img_tensor = torch.stack(tensors) \n",
    "    \n",
    "# Create label (Y) tensor\n",
    "class_tensor = torch.tensor(img_class)  \n",
    "\n",
    "# Get look at rescaled values\n",
    "print(f'After rescaling, min pixel value: {img_tensor.min().item()} and max pixel value: {img_tensor.max().item()}')\n",
    "\n",
    "\n",
    "# do this for testing\n",
    "tensors_test = list()\n",
    "for img_path in img_files_test:\n",
    "    PIL_img = Image.open(img_path)\n",
    "    # from PIL image, get a tensor\n",
    "    tensor_img = toTensor(PIL_img)\n",
    "    \n",
    "    # the image isn't empty, rescale its values to run from 0 to 1\n",
    "    if(tensor_img.min() < tensor_img.max()):  \n",
    "        tensor_img = (tensor_img - tensor_img.min()) / (tensor_img.max() - tensor_img.min()) \n",
    "    \n",
    "    # append list of tensors\n",
    "    tensors_test.append(tensor_img)\n",
    "    \n",
    "# stack image (X) tensor\n",
    "img_tensor_test = torch.stack(tensors_test) \n",
    "    \n",
    "# Create label (Y) tensor\n",
    "class_tensor_test = torch.tensor(img_class_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d675c4-c38f-4fb2-b18f-9e041ab65a73",
   "metadata": {},
   "source": [
    " - vytvor√≠me triedu n√°≈°ho modelu zalo≈æen√©ho na konvoluƒçnej umelej neur√≥novej sieti pomocou modulu PyTorch, priƒçom v≈°etky v√Ωpoƒçty bud√∫ rozdelen√© na grafickej jendotke pomocou architekt√∫ry CUDA, nevyhnutn√©, aby grafick√° karta a stroj podporovali CUDA architekt√∫ru  \n",
    " - ako aktivaƒçn√∫ funkciu sme vybrali RELU, ktor√° nepredpoklad√° z√°porn√© hodnoty, m√¥≈æme ju definova≈• ako __f(x) = max(0, x)__, kde __x__ je n√°≈° vstup, vzhƒæadom na to, ≈æe m√°me hodnoty pixelov od 0 po 255 vo farebnom modely grayscale, tak nemus√≠me pracova≈• so z√°porn√Ωmi hodnotami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7ebabd-4113-473d-b49c-26cd9e861b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_ANN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    :param torch.nn.Module: Base class for all neural network modules.\n",
    "    \n",
    "    Based on the architecture of the convolutional neural network with RELU.\n",
    "    The number of labels = the output layer nodes.\n",
    "    \n",
    "    This model work only with images with 1 channel, so in grayscale color space for this particular assignment.\n",
    "    \"\"\"\n",
    "    convolution = (5, 10)\n",
    "    convolution_size = 7\n",
    "     \n",
    "    def __init__(self, img_width, img_height, no_labels): \n",
    "        \"\"\"\n",
    "        :param self: object\n",
    "        :param img_width: Image's x dimension\n",
    "        :param img_height: Image's y dimension\n",
    "        :param no_labels: Number of labels\n",
    "        \n",
    "        Initialize a model based on the architecture of the convolutional neural network \n",
    "        with Rectified Linear Unit activation function. \n",
    "        \n",
    "        We apply convolution tuple to pixels in our model, while in the convolution convolution_size is the number of surrounding pixels.\n",
    "        Formula input_layer_no_nodes calculates last node in order to connect layers.\n",
    "        \n",
    "        Image's height with image's width decreases with each convolution, because it makes image smaller. \n",
    "        \"\"\"       \n",
    "\n",
    "        # Set constructor of torch.nn.Module in order set up the CNN_ANN class\n",
    "        super(CNN_ANN,self).__init__()  \n",
    "\n",
    "        # Set number of nodes for the input layer.\n",
    "        self.input_layer_no_nodes = self.convolution[1]*(img_width-(self.convolution_size-1)-(self.convolution_size-1))*(img_height-(self.convolution_size-1)-(self.convolution_size-1))\n",
    "   \n",
    "        # Store weights between layers\n",
    "        # Override atribures for base class.\n",
    "        # nn.Conv2d(channels in, channels out, convolution height/width)        \n",
    "        no_channels = 1 # grayscale, for RGB we would have 3 \n",
    "        self.cnv1 = torch.nn.Conv2d(no_channels, self.convolution[0], self.convolution_size)\n",
    "        self.cnv2 = torch.nn.Conv2d(self.convolution[0], self.convolution[1], self.convolution_size)\n",
    "\n",
    "\n",
    "        # Define the number of output nodes for fully connected layers\n",
    "        fully_connected_layer1_no_nodes = 400\n",
    "        fully_connected_layer2_no_nodes = 80\n",
    "        \n",
    "        # Stores the weights between the fully connected layers\n",
    "        # nn.Linear(nodes in, nodes out)\n",
    "        self.weight_full_connected_layer1 = torch.nn.Linear(self.input_layer_no_nodes, fully_connected_layer1_no_nodes)\n",
    "        self.weight_full_connected_layer2 = torch.nn.Linear(fully_connected_layer1_no_nodes, fully_connected_layer2_no_nodes)\n",
    "        self.weight_full_connected_layer_final = torch.nn.Linear(fully_connected_layer2_no_nodes, no_labels)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        :param self: An object\n",
    "        :param x: Input layer.\n",
    "        :returns x: Output layer.\n",
    "        \n",
    "        Define how to get the output of the neural network. Needs to be implemented. \n",
    "        This is called when the neural network is applied to an input.\n",
    "        \n",
    "        Define the steps used in the computation of output from input.\n",
    "        Use the weights defined in the constructor.\n",
    "        We use RELU activaciton function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply RELU activation fuction for first convolutional layer \n",
    "        x = torch.nn.functional.relu(self.cnv1(x)) \n",
    "        \n",
    "        # Apply RELU activation fuction for second convolutional layer\n",
    "        x = torch.nn.functional.relu(self.cnv2(x)) \n",
    "        \n",
    "        # Connect convolutional layer into connected layer\n",
    "        x = x.view(-1, self.num_flat_features(x)) \n",
    "        \n",
    "        # Apply RELU activation fuction for first connected layer\n",
    "        x = torch.nn.functional.relu(self.weight_full_connected_layer1(x)) \n",
    "        \n",
    "        # Apply RELU activation fuction for second connected layer\n",
    "        x = torch.nn.functional.relu(self.weight_full_connected_layer2(x)) \n",
    "        \n",
    "        # Final connected layer without RELU, calculate loss function\n",
    "        x = self.weight_full_connected_layer_final(x)        \n",
    "        \n",
    "        # Return final connected layer\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):  \n",
    "        \"\"\"\n",
    "        :param self: An object\n",
    "        :param x: Input layer\n",
    "        : returns \n",
    "        \n",
    "        Change x to a vector. Needs to be explicitly implemented.\n",
    "        \"\"\"\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8fdbf0-4e23-4bc8-b1ac-d3b1720d54c2",
   "metadata": {},
   "source": [
    " - vytvor√≠me in≈°tanciu n√°≈°ho modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5d16d4-a487-4ca1-ac36-ec61a3a76d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model object and pass all calculations to the GPU with CUDA architecture\n",
    "model = CNN_ANN(img_width, img_height, no_classes).to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209950ff-e087-4b45-a535-72e0aef1e77d",
   "metadata": {},
   "source": [
    " - vytvor√≠me zoznam obr√°zkov pre tr√©novanie a testovanie, 47163 obr√°zkov m√°me pre tr√©novanie a 11791 pre testovanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef8f14cc-253a-4350-be0e-2ef8bb89fa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images = 47163 Testing = 11791\n"
     ]
    }
   ],
   "source": [
    "test_img_numbers = list()\n",
    "train_img_numbers = list()\n",
    "\n",
    "# Add imgs into train list\n",
    "for i in range(no_total_img):\n",
    "        train_img_numbers.append(i)\n",
    "        \n",
    "# Add imgs into test list\n",
    "for i in range(no_total_img_train):\n",
    "        test_img_numbers.append(i)\n",
    "        \n",
    "# Count the number in each set\n",
    "no_img_train = len(train_img_numbers)  \n",
    "no_img_test = len(test_img_numbers)\n",
    "print(\"Training images =\", no_img_train, \"Testing =\", no_img_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501a11f-8817-4552-a988-3f6b9bd614c2",
   "metadata": {},
   "source": [
    " - vytvor√≠me tensor d√°tov√© typy pre tr√©novanie a testovanie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcacf545-62d9-4515-9354-15e52795a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = torch.tensor(train_img_numbers)    \n",
    "test_ids = torch.tensor(test_img_numbers)\n",
    "trainX = img_tensor[train_ids,:,:,:]\n",
    "trainY = class_tensor[train_ids]\n",
    "testX = img_tensor_test[test_ids,:,:,:]\n",
    "testY = class_tensor_test[test_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b2078-846b-4c9a-94d4-e63167754ed1",
   "metadata": {},
   "source": [
    " - natr√©nujeme na≈°u neur√≥novu sie≈• modelu v 10 epoch√°ch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5dc6451-15d0-4cf4-80cd-84afb3ca3aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0; Training loss = 146.43627820909023\n",
      "Epoch = 1; Training loss = 26.841743130236864\n",
      "Epoch = 2; Training loss = 21.40828127786517\n",
      "Epoch = 3; Training loss = 10.7601820230484\n",
      "Epoch = 4; Training loss = 7.677139786072075\n",
      "Epoch = 5; Training loss = 6.502034819684923\n",
      "Epoch = 6; Training loss = 19.35174631047994\n",
      "Epoch = 7; Training loss = 5.856186275370419\n",
      "Epoch = 8; Training loss = 4.208227928727865\n",
      "Epoch = 9; Training loss = 3.2181634549051523\n"
     ]
    }
   ],
   "source": [
    "# Setting too large batch size will cause an out-of-memory error.\n",
    "batch_size = 300           \n",
    "\n",
    "# Number of training batches per epoch, round down to simplify last batch\n",
    "trainBats = no_img_train // batch_size       \n",
    "\n",
    "# Test batches, round up to include all\n",
    "testBats = -(-no_img_test // batch_size)     \n",
    "\n",
    "# Take into the account the imbalanced dataset.\n",
    "weights = torch.zeros(no_classes)     \n",
    "\n",
    "# Rarer images are counted more to the loss, so we prevent the model from ignoring rarer images.\n",
    "for i in trainY.tolist():            \n",
    "    weights[i].add_(1)              \n",
    "    \n",
    "# Weights should be inversely related to count\n",
    "weights = 1. / weights.clamp_(min=1.)                     \n",
    "\n",
    "# The weights average to 1, send to CUDA\n",
    "weights = (weights * no_classes / weights.sum()).to(dev)\n",
    "\n",
    "# Initialize an optimizer, lr means learning rate\n",
    "optimizerSGD = torch.optim.SGD(model.parameters(), lr = 0.01)   \n",
    "\n",
    "# Max training epochs = 10\n",
    "for epoch in range(0, 10, 1):\n",
    "    # Set model to training mode\n",
    "    model.train()                     \n",
    "    loss_epoch = 0.\n",
    "    # Shuffle data to randomize batches\n",
    "    permute = torch.randperm(no_img_train)  \n",
    "    trainX = trainX[permute,:,:,:]\n",
    "    trainY = trainY[permute]\n",
    "    # Iterate over batches\n",
    "    for j in range(trainBats):        \n",
    "        # Zero out gradient accumulated in optimizer\n",
    "        optimizerSGD.zero_grad()              \n",
    "        # Slice shuffled data into batches\n",
    "        batX = trainX[j*batch_size:(j+1)*batch_size,:,:,:].to(dev)   \n",
    "        # .to(dev) moves these batches to the GPU\n",
    "        batY = trainY[j*batch_size:(j+1)*batch_size].to(dev)    \n",
    "        # Evalute predictions\n",
    "        yOut = model(batX)            \n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.cross_entropy(yOut, batY,weight=weights) \n",
    "        # Add loss\n",
    "        loss_epoch += loss.item()   \n",
    "        # Backpropagate loss\n",
    "        loss.backward()          \n",
    "        # Update model weights using optimizer\n",
    "        optimizerSGD.step()    \n",
    "    print(f\"Epoch = {epoch}; Training loss = {loss_epoch}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50102201-fd75-4489-bc29-f5844b13ba7f",
   "metadata": {},
   "source": [
    " - n√°≈° model mal korektn√© predikcie na testovan√Ωch d√°tach 11706 / 11791, ƒço predstavuje pribli≈æne 99% √∫spe≈°nos≈•:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad18c87d-18ec-49e4-8cf9-cf138dbea70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions:  11706 / 11791\n",
      "Confusion Matrix:\n",
      " [[1988    2    6    1    0    3]\n",
      " [   0 1990    4    1    5    0]\n",
      " [   5    7 1982    0    6    0]\n",
      " [   0    0    0 1789    2    0]\n",
      " [   0    1    2    3 1994    0]\n",
      " [  37    0    0    0    0 1963]]\n"
     ]
    }
   ],
   "source": [
    "# Set an empty confusion matrix\n",
    "confusion_matrix = np.zeros((no_classes,no_classes),dtype=int)    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Shuffle test data\n",
    "    permute = torch.randperm(no_img_test)                     \n",
    "    testX = testX[permute,:,:,:]\n",
    "    testY = testY[permute]\n",
    "    \n",
    "    # Iterate over test batches\n",
    "    for j in range(testBats):                           \n",
    "        batX = testX[j*batch_size:(j+1)*batch_size,:,:,:].to(dev)\n",
    "        batY = testY[j*batch_size:(j+1)*batch_size].to(dev)\n",
    "        \n",
    "        # Pass test batch through model\n",
    "        yOut = model(batX)                 \n",
    "        \n",
    "        # Generate predictions by finding the max Y values\n",
    "        pred = yOut.max(1,keepdim=True)[1]     \n",
    "        \n",
    "        # Bind actual and predicted to\n",
    "        for j in torch.cat((batY.view_as(pred), pred),dim=1).tolist(): \n",
    "            # make (row, col) pairs, increment confusion matrix\n",
    "            confusion_matrix[j[0],j[1]] += 1   \n",
    "            \n",
    "# Sum over diagonal elements to count correct predictions\n",
    "correct_predictions = 0\n",
    "for i in range(0, no_classes, 1):\n",
    "    correct_predictions += confusion_matrix[i,i] \n",
    "print(\"Correct predictions: \", correct_predictions, \"/\", no_img_test)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3985d-b23b-4616-bb19-3065242bc29a",
   "metadata": {},
   "source": [
    "## Z√°ver\n",
    "\n",
    " - pre implement√°ciu modelu konvoluƒçnej neur√≥novej sieti sme sa rozhodli pou≈æi≈• Python modul PyTorch s vyu≈æit√≠m CUDA architekt√∫ry pre paraleliz√°ciu v√Ωpoƒçtov na GPU\n",
    " - √∫spe≈°nos≈• n√°≈°ho modelu je pribli≈æne 99%, vo viacer√Ωch sp√∫≈°≈•aniach sa n√°m √∫spe≈°nos≈• pohybovala medzi 97% a≈æ 99%\n",
    " - model by sa dal zlep≈°i≈• analyzovan√≠m obr√°zkov, ktor√© model nevedel spr√°vne vyhodnoti≈•\n",
    " - na na≈°ich d√°tach sme nerie≈°ili overfitting(model je pr√≠li≈° natr√©novan√Ω na tr√©novacie d√°ta) ani underfitting(model nevie zachyti≈• ≈°trukt√∫ru d√°t), do bud√∫cna by bolo vhodn√© zlep≈°i≈• tento model valid√°ciou na overfitting\n",
    " - ƒèal≈°√≠m zlep≈°en√≠m by mohla by≈• vlastn√° loss funkcia, ktor√° by mohla zlep≈°i≈• predikcie modelu\n",
    " - ako alternat√≠vu ku konvoluƒçnej neur√≥novej sieti by sme mohli zv√°≈æi≈• UNet alebo UNet++, ƒço s√∫ neur√≥nove siete zalo≈æen√© na konvoluƒçnej neur√≥novej sieti ≈°peci√°lne pre medic√≠nske obr√°zky\n",
    " - ako ƒèal≈°ia alternat√≠va pripad√° do √∫vahy kombin√°cia spom√≠nan√Ωch neur√≥nov√Ωch sieti - konvoluƒçnej a UNet, ktor√° sa naz√Ωva aj ako kaskadov√° konvoluƒçn√° sie≈• s UNet, ƒço sa pou≈æ√≠va pri segmentovan√≠ a klasifikovan√≠ buniek v medic√≠nskych d√°tach, av≈°ak v na≈°om pr√≠pade nem√°me d√°ta s bunkami, tak v tomto pr√≠pade, by v√Ωsledky nemuseli by≈• lep≈°ie\n",
    " \n",
    "## Bibliography\n",
    "\n",
    "[1] Medical MNIST Classification, apolanco3225, 2017, GitHub, GitHub repository, https://github.com/apolanco3225/Medical-MNIST-Classification\n",
    "\n",
    "[2] Exploratory Data Analysis Ideas for Image Classification, Eunjoo Byeon, towards data science, article, https://towardsdatascience.com/exploratory-data-analysis-ideas-for-image-classification-d3fc6bbfb2d2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
